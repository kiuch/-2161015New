import pandas as pd
import MeCab
from sklearn.feature_extraction.text import TfidfVectorizer
import os
import matplotlib.pyplot as plt
from collections import Counter 

# è¤‡æ•°ã®ãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã®è¨­å®š
file_config = [
    {'title': 'SV', 'path': r'C:\Users\masat\OneDrive\ãƒ‡ã‚¹ã‚¯ãƒˆãƒƒãƒ—\deep learning\ãƒ‘ãƒ¯ãƒ\-2161015New\SVãƒ¬ãƒ“ãƒ¥ãƒ¼æ–‡.csv', 'review_col': 'ãƒ¬ãƒ“ãƒ¥ãƒ¼'},
    {'title': 'å‰£ç›¾', 'path': r'C:\Users\masat\OneDrive\ãƒ‡ã‚¹ã‚¯ãƒˆãƒƒãƒ—\deep learning\ãƒ‘ãƒ¯ãƒ\-2161015New\å‰£ç›¾ã‚·ãƒŠãƒªã‚ªæ–‡.csv', 'review_col': 'ã‚·ãƒŠãƒªã‚ªä¸€æ–‡'},
    {'title': 'USUM', 'path': r'C:\Users\masat\OneDrive\ãƒ‡ã‚¹ã‚¯ãƒˆãƒƒãƒ—\deep learning\ãƒ‘ãƒ¯ãƒ\-2161015New\sm_usumã‚·ãƒŠãƒªã‚ªæ–‡.csv', 'review_col': 'ã‚·ãƒŠãƒªã‚ªæ–‡'}
]

# ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ (å½¢æ…‹ç´ è§£æã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã«ä½¿ç”¨)
stop_words = {
    "ã“ã®", "ã®", "ã¯", "ãŒ", "ã«", "ã‚’", "ã¨", "ã¦", "ãŸ", "ã ", "ã—", "ã‚‚ã£ã¨", "ã‚‚", "ã§ã™", "ã¾ã™", "ã‘ã©", "ã ã‚", "ãã‚Œ", 
    "ã„ã†", "ã‚ã‚‹", "ã‚‚ã®", "ãªã‚‹", "ã™ã‚‹", "ã„ã‚‹", "ã“ã¨", "ãªã„", "ã§ãã‚‹", "ãŸã‚", "ããƒ", "ã‚‰ã‚Œã‚‹", "ã‚Œã‚‹", "ã“ã‚Œ", 
    "ã‚¹ãƒ«", "ã‚¤ãƒ«", "ã‚¤ã‚¦", "ã‚¢ãƒ«", "ãƒŠãƒ«", "ãƒŠã‚¤", "ã‚³ãƒˆ", "ãƒ‡ã‚­ãƒ«", "ã‚·ãƒ¬ãƒ«", "ã‚«ãƒ³ã‚ºãƒ«", "ãƒ¢ãƒ",
    "ã‚²ãƒ¼ãƒ ", "ã‚·ãƒªãƒ¼ã‚º", "ãƒã‚±ãƒ¢ãƒ³", "ãƒ›ãƒ³ã‚µã‚¯", "ãƒ«ãƒ¼ãƒˆ", "ãƒ–ãƒ–ãƒ³", 
    "ãƒ¬ãƒ™ãƒ«", "ã‚¿ãƒ¡", "ã‚½ãƒ", "ã‚»ã‚¤ãƒªãƒ„", "ãƒˆã‚ªã‚¯", "ãƒŸã‚¨ãƒ«", "ãƒãƒ„", "ã‚¤ã‚¯", "ã‚¯ãƒ«", "ã‚ªã‚¯", 
    "ãƒ›ã‚«ã‚¯", "ã‚·ãƒ¥ãƒ«ã‚¤","ãƒãƒ", "ã‚¤ãƒ", "ã‚¢ã‚¿ãƒª", "ãƒã‚¢ã‚¤", "ã‚¸ãƒ ", "è¦ç´ ", "ã‚·ã‚¹ãƒ†ãƒ ", 
    "æ„Ÿæƒ³", "ç‚¹", "éƒ¨åˆ†", "ä»Šå›", "æ„Ÿã˜", "æ€ã£ãŸ", "ã¨ã“ã‚", "ã¾ãŸ" 
}

# MeCab Taggerã®åˆæœŸåŒ–
try:
    mecab = MeCab.Tagger() 
except Exception as e:
    print(f"MeCabã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
    exit()

# Matplotlibã®æ—¥æœ¬èªè¨­å®š
plt.rcParams['font.family'] = 'Meiryo' 
plt.rcParams['font.size'] = 12

# --- 2. ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•°ï¼ˆãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ãƒ»å‰å‡¦ç†ï¼‰ ---

def force_read_csv(file_path):
    """è¤‡æ•°ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’è©¦ã—ã¦CSVã‚’èª­ã¿è¾¼ã‚€"""
    encodings_to_try = ['utf-8', 'shift_jis', 'cp932', 'euc-jp']
    for encoding in encodings_to_try:
        try:
            df = pd.read_csv(file_path, encoding=encoding)
            return df
        except Exception:
            continue
    try:
        df = pd.read_csv(file_path, encoding='utf-8', errors='ignore')
        return df
    except Exception:
        return None

def preprocess_text(text, mecab_tagger):
    """ãƒ†ã‚­ã‚¹ãƒˆã‚’å½¢æ…‹ç´ è§£æã—ã€åè©ãƒ»å‹•è©ãƒ»å½¢å®¹è©ãƒ»æ„Ÿå‹•è©ã®åŸå½¢ã‚’æŠ½å‡º"""
    words = []
    if not isinstance(text, str) or len(text) < 2:
        return []
        
    target_hinshi = ('åè©', 'å‹•è©', 'å½¢å®¹è©', 'æ„Ÿå‹•è©')
    
    try:
        node = mecab_tagger.parseToNode(text)
    except Exception:
        return []

    while node:
        features = node.feature.split(',')
        hinshi = features[0]
        
        original_form_for_check = node.surface
        if len(features) >= 7 and features[6] != '*':
            original_form_for_check = features[6]

        surface_form = node.surface
        
        # NOTE: n-gramç”Ÿæˆã‚’å¤–éƒ¨ã§è¡Œã†ãŸã‚ã€ã“ã“ã§ã¯å˜èªãƒªã‚¹ãƒˆã‚’ç”Ÿæˆã™ã‚‹
        if hinshi in target_hinshi and original_form_for_check not in stop_words and len(surface_form) > 1:
            words.append(surface_form)
        
        node = node.next
    return words

def generate_ngrams(token_list, n_gram=1):
    # TF-IDFã¯å˜èªãƒªã‚¹ãƒˆã§ã¯ãªãã€ã‚¹ãƒšãƒ¼ã‚¹åŒºåˆ‡ã‚Šã®æ–‡å­—åˆ—ã‚’å¿…è¦ã¨ã™ã‚‹
    token = [t for t in token_list if t != ""] 
    if not token:
        return []
        
    ngrams = zip(*[token[i:] for i in range(n_gram)])
    return [" ".join(ngram) for ngram in ngrams]


def extract_feature_words(terms, tfidfs, i, n):
    # tfidfsã¯å¯†è¡Œåˆ—ï¼ˆtoarray()å¾Œï¼‰
    tfidf_array = tfidfs[i]
    top_n_idx = tfidf_array.argsort()[-n:][::-1]
    words = [terms[idx] for idx in top_n_idx]
    scores = [tfidf_array[idx] for idx in top_n_idx]
    return list(zip(words, scores))
def main():
    print("TF-IDFã‚’ç”¨ã„ãŸä½œå“é–“ç‰¹å¾´èªæŠ½å‡ºã‚’é–‹å§‹ã—ã¾ã™...")

    if not os.path.exists('results'):
        os.makedirs('results')

    titles = [c['title'] for c in file_config]
    combined_reviews_by_title = {}
   
    for config in file_config:
        title = config['title']
        path = config['path']
        review_col = config['review_col']
        
        print(f"\n==================== {title} ã®å‰å‡¦ç†ã‚’é–‹å§‹ ====================")
        
        df = force_read_csv(path)
        df_game = df.copy()
        df_game = df_game.rename(columns={review_col: 'Original_Review'})
        df_game['Original_Review'] = df_game['Original_Review'].astype(str).str.strip().replace('nan', '')
        df_game = df_game[df_game['Original_Review'].str.len() > 1]
        game_reviews = df_game['Original_Review'].tolist()
        
        # å½¢æ…‹ç´ è§£æã¨ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        processed_reviews = [preprocess_text(review, mecab) for review in game_reviews]
        
        all_ngrams = []
        for review in processed_reviews:
             # TF-IDFã®ãŸã‚ã«ã€n-gramç”Ÿæˆï¼ˆå˜èªãƒªã‚¹ãƒˆã‚’ã‚¹ãƒšãƒ¼ã‚¹åŒºåˆ‡ã‚Šæ–‡å­—åˆ—ã«æˆ»ã™ï¼‰
            all_ngrams.extend(generate_ngrams(review, n_gram=1)) 
            
        combined_reviews_by_title[title] = " ".join(all_ngrams)
        print(f"âœ… {title} ã®ãƒ¬ãƒ“ãƒ¥ãƒ¼çµåˆä½“ã‚’ä½œæˆã—ã¾ã—ãŸã€‚ï¼ˆç·å˜èªæ•°: {len(all_ngrams)}ï¼‰")

    document_list = [combined_reviews_by_title[title] for title in titles]
    
    tfidf_vectorizer = TfidfVectorizer(
        min_df = 0.0, 
        ngram_range=(1, 2) 
    )

    tfidf_matrix = tfidf_vectorizer.fit_transform(document_list)
    terms = tfidf_vectorizer.get_feature_names_out()
    tfidfs = tfidf_matrix.toarray()

    print("\n==================== ğŸ“ˆ TF-IDFè¡Œåˆ—ã®è¨ˆç®—å®Œäº† ====================")
    print(f"âœ… åˆ†æå¯¾è±¡ã®N-gramæ•°ã¯ {len(terms)} ç¨®é¡ã§ã™ã€‚")

    
    n_features = 50 # å„ä½œå“ã§ä¸Šä½50å€‹ã®ç‰¹å¾´èªã‚’æŠ½å‡º
    all_feature_data = []

    print(f"\n==================== ğŸ—ï¸ ä½œå“åˆ¥ ç‰¹å¾´èªãƒ©ãƒ³ã‚­ãƒ³ã‚° (ä¸Šä½{n_features}èª) ====================")
    
    for i, title in enumerate(titles):
        feature_words_scores = extract_feature_words(terms, tfidfs, i, n_features)
        
        df_feature = pd.DataFrame(feature_words_scores, columns=['Feature_Word_Ngram', 'TFIDF_Score'])
        df_feature['Game_Title'] = title
        df_feature['Rank'] = range(1, len(df_feature) + 1)
        all_feature_data.append(df_feature)
        
        print(f"\n--- {title} ã®ç‰¹å¾´èª ---")
        print(df_feature[['Rank', 'Feature_Word_Ngram', 'TFIDF_Score']].head(10))

    # å…¨çµæœã‚’çµ±åˆã—ã¦CSVå‡ºåŠ›
    df_all_features = pd.concat(all_feature_data, ignore_index=True)
    output_path = 'results/tfidf_key_feature_words.csv'
    df_all_features.to_csv(output_path, index=False, encoding='utf-8')
    
    print(f"\nâœ… å…¨ä½œå“ã®ç‰¹å¾´èªï¼ˆä¸Šä½{n_features}èªï¼‰ã‚’ '{output_path}' ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")
  
    for title in titles:
        df_plot = df_all_features[df_all_features['Game_Title'] == title].head(10)
        
        plt.figure(figsize=(10, 6))
        # TF-IDFã‚¹ã‚³ã‚¢ã«åŸºã¥ã„ã¦æ£’ã‚°ãƒ©ãƒ•ã‚’ä½œæˆ
        plt.barh(df_plot['Feature_Word_Ngram'], df_plot['TFIDF_Score'], color='#4682B4')
        plt.title(f'{title} ã‚’æœ€ã‚‚ç‰¹å¾´ã¥ã‘ã‚‹å˜èª (TF-IDF Top 10)', fontsize=14)
        plt.xlabel('TF-IDF Score')
        plt.ylabel('å˜èª / N-gram')
        # ã‚°ãƒ©ãƒ•ã‚’é€†é †ã«ã—ã¦ã€é•·ã„å˜èªã‚‚è¡¨ç¤ºå¯èƒ½ã«ã™ã‚‹
        plt.gca().invert_yaxis() 
        plt.tight_layout()
        plt.savefig(f'results/{title}_tfidf_top10_features.png')
        plt.close()
        print(f"âœ… {title} ã®TF-IDFæ£’ã‚°ãƒ©ãƒ•ã‚’ä¿å­˜ã—ã¾ã—ãŸã€‚")

    print("\n--- å…¨å‡¦ç†ã‚’å®Œäº†ã—ã¾ã—ãŸ ---")

if __name__ == "__main__":
    main()