import pandas as pd
import MeCab
import numpy as np
from collections import Counter, defaultdict
from sklearn.feature_extraction.text import TfidfVectorizer
import matplotlib.pyplot as plt

# --- 1. æº–å‚™ã¨è¨­å®š ---

# æ—¥æœ¬èªè¡¨ç¤ºã®è¨­å®š (Windowsç’°å¢ƒã§åˆ©ç”¨å¯èƒ½ãªãƒ•ã‚©ãƒ³ãƒˆã‚’å„ªå…ˆ)
# ã‚‚ã—Meiryoã§ã‚¨ãƒ©ãƒ¼ãŒå‡ºã‚‹å ´åˆã¯ 'Yu Gothic' ã‚„ 'MS Gothic' ã‚’è©¦ã—ã¦ãã ã•ã„ã€‚
plt.rcParams['font.family'] = 'Meiryo' 
plt.rcParams['font.size'] = 12

# ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ãƒ‘ã‚¹
file_path = r'C:\Users\masat\OneDrive\ãƒ‡ã‚¹ã‚¯ãƒˆãƒƒãƒ—\deep learning\ãƒã‚±ãƒ¢ãƒ³svã‚·ãƒŠãƒªã‚ª1.csv'

# æ„Ÿæƒ…æ¥µæ€§è¾æ›¸ï¼ˆã‚·ãƒŠãƒªã‚ªè©•ä¾¡ã«ç‰¹åŒ–ã—ã¦å¼·åŒ–ï¼‰
positive_words = {"ç´ æ™´ã‚‰ã—ã„", "æ„Ÿå‹•çš„", "æœ€é«˜", "åä½œ", "é¢ç™½ã„", "è‰¯ã„", "è‰¯ã‹ã£ãŸ", "å¥½ã", "æ³£ã", "ç¥", "æ¥½ã—ã„", "æœŸå¾…", "ã‚«ãƒ¯ã‚¤ã‚¤", "ãƒ„ãƒŠã‚¬ãƒ«", "ã‚¿ãƒã‚·ã‚¤", "ã‚¢ãƒ„ã„", "ãƒ†ãƒ³ã‚«ã‚¤"}
negative_words = {"å¼±ã„", "å¹³å‡¡", "æ®‹å¿µ", "é™³è…", "æœ€æ‚ª", "ã‚¹ãƒˆãƒ¬ã‚¹", "è©•ä¾¡ã§ããªã„", "å¾®å¦™", "ã¤ã¾ã‚‰ãªã„", "ä¸æº€", "æ‚ªã„", "ã‚ªã‚¯ãƒ¬", "ãƒ¢ãƒ³ãƒ€ã‚¤", "ãƒ ãƒªãƒ§ã‚¦", "ã‚½ã‚¬ã‚¤", "ãƒ¡ãƒ³ãƒ‰ã‚¦", "ã‚µã‚¤ã‚¢ã‚¯", "ã‚³ãƒ³ãƒŠãƒ³", "ãƒ¯ãƒ«ã‚¤", "ãƒŠãƒ³ã‚¤"}

# ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ (é »å‡ºã™ã‚‹ãŒæ„å‘³ã®è–„ã„å˜èªã‚’å¤§å¹…ã«è¿½åŠ ãƒ»å¼·åŒ–)
stop_words = {
    # è‹±èªã®ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ã‚’å¾¹åº•æ’é™¤
    "the", "is", "it", "and", "to", "that", "of", "are", "in", "this", "but", "The", "game", 
    "for", "you", "they", "we", "can", "have", "not", "will",
    "ã“ã®", "ã®", "ã¯", "ãŒ", "ã«", "ã‚’", "ã¨", "ã¦", "ãŸ", "ã ", "ã—", "ã‚‚ã£ã¨", "ã‚‚", "ã§ã™", "ã¾ã™", "ã‘ã©", "ã ã‚", "ãã‚Œ", 
    "ã„ã†", "ã‚ã‚‹", "ãªã‚‹", "ã™ã‚‹", "ã„ã‚‹", "ã“ã¨", "ãªã„", "ã§ãã‚‹", "ã‚‚ã®", "ãŸã‚", "ããƒ", "ã‚‰ã‚Œã‚‹", "ã‚Œã‚‹", "ã“ã‚Œ", 
    # ã‚«ã‚¿ã‚«ãƒŠã®è£œåŠ©å‹•è©ãƒ»å‹•è©ãƒ»åŠ©å‹•è©
    "ã‚¹ãƒ«", "ã‚¤ãƒ«", "ã‚¤ã‚¦", "ã‚¢ãƒ«", "ãƒŠãƒ«", "ãƒŠã‚¤", "ã‚³ãƒˆ", "ãƒ‡ã‚­ãƒ«", "ã‚·ãƒ¬ãƒ«", "ã‚«ãƒ³ã‚ºãƒ«", 
    # ãƒ‡ãƒ¼ã‚¿ã§é »å‡ºã—ãŸæ±ç”¨çš„ãªå˜èª
    "ã‚²ãƒ¼ãƒ ", "ã‚·ãƒªãƒ¼ã‚º", "ãƒã‚±ãƒ¢ãƒ³", "ãƒ¯ãƒ¼ãƒ«ãƒ‰", "ã‚ªãƒ¼ãƒ—ãƒ³", "ãƒ›ãƒ³ã‚µã‚¯", "ãƒ—ãƒ¬ãƒ¼ãƒ¤ãƒ¼", "ãƒ«ãƒ¼ãƒˆ", "ãƒ–ãƒ–ãƒ³", 
    "ãƒ¬ãƒ™ãƒ«", "ã‚¿ãƒ¡", "ã‚½ãƒ", "ã‚»ã‚¤ãƒªãƒ„", "ãƒˆã‚ªã‚¯", "ãƒŸã‚¨ãƒ«", "ãƒãƒ„", "ã‚¤ã‚¯", "ã‚¯ãƒ«", "ã‚ªã‚¯", "ãƒ–ã‚¿ã‚¤", "ã‚«ãƒ³ã‚±ã‚¤",
    "ãƒ›ã‚«ã‚¯", "ã‚·ãƒ¥ãƒ«ã‚¤", "ã‚¿ãƒãƒ", "ãƒãƒ", "ã‚¤ãƒ", "ã‚¢ã‚¿ãƒª", "ãƒã‚¢ã‚¤"
}

# ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿é–¢æ•° (å‰å›ã®å›ç­”ã§ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã®å•é¡Œã¯è§£æ±ºæ¸ˆã¿ã¨ä»®å®š)
def force_read_csv(file_path):
    encodings_to_try = ['utf-8', 'shift_jis', 'cp932', 'euc-jp']
    for encoding in encodings_to_try:
        try:
            df = pd.read_csv(file_path, encoding=encoding)
            return df
        except Exception:
            continue
    try:
        df = pd.read_csv(file_path, encoding='utf-8', errors='ignore')
        return df
    except Exception:
        return None

df = force_read_csv(file_path)

# ãƒ¬ãƒ“ãƒ¥ãƒ¼æœ¬æ–‡ãŒå«ã¾ã‚Œã‚‹åˆ—åã‚’æ˜ç¤ºçš„ã«æŒ‡å®š
TEXT_COLUMN = 'ãƒ¬ãƒ“ãƒ¥ãƒ¼' 

if TEXT_COLUMN in df.columns:
    game_reviews = df[TEXT_COLUMN].astype(str).tolist()
    game_reviews = [r.replace('nan', '').strip() for r in game_reviews if r != 'nan' and r != '']
    print(f"âœ… ãƒ¬ãƒ“ãƒ¥ãƒ¼åˆ—: '{TEXT_COLUMN}' ã‚’åˆ†æå¯¾è±¡ã¨ã—ã¾ã™ã€‚")
else:
    print(f"ğŸš¨ ã‚¨ãƒ©ãƒ¼: ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã« '{TEXT_COLUMN}' ã¨ã„ã†åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
    raise ValueError(f"åˆ— '{TEXT_COLUMN}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")

# --- MeCab Taggerã®åˆæœŸåŒ– ---
mecab = MeCab.Tagger() 

def preprocess_text(text):
    """ãƒ†ã‚­ã‚¹ãƒˆã‚’å½¢æ…‹ç´ è§£æã—ã€åè©ãƒ»å‹•è©ãƒ»å½¢å®¹è©ãƒ»æ„Ÿå‹•è©ã®åŸå½¢ã‚’æŠ½å‡º"""
    words = []
    if not isinstance(text, str) or len(text) < 2:
        return []
        
    try:
        node = mecab.parseToNode(text)
    except Exception:
        return []

    target_hinshi = ('åè©', 'å‹•è©', 'å½¢å®¹è©', 'æ„Ÿå‹•è©')
    
    while node:
        features = node.feature.split(',')
        hinshi = features[0]
        
        # 1. ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ãƒã‚§ãƒƒã‚¯ç”¨ã®åŸå½¢ (å¤šãã®å ´åˆã‚«ã‚¿ã‚«ãƒŠ) ã‚’å–å¾—
        original_form_for_check = node.surface
        if len(features) >= 7 and features[6] != '*':
            original_form_for_check = features[6]

        # 2. çµæœå‡ºåŠ›ç”¨ã®è¡¨é¢å½¢ï¼ˆå…ƒã®è¡¨è¨˜ï¼‰ã‚’å–å¾—
        surface_form = node.surface
        
        # æŠ½å‡ºæ¡ä»¶: 
        # 1. å¯¾è±¡å“è©ã§ã‚ã‚‹ã“ã¨
        # 2. åŸå½¢ãŒã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ã«å«ã¾ã‚Œãªã„ã“ã¨ (ã‚«ã‚¿ã‚«ãƒŠã®ã€Œã‚¹ãƒ«ã€ã‚’ã“ã‚Œã§é˜²ã)
        # 3. è¡¨é¢å½¢ãŒ1æ–‡å­—ã‚ˆã‚Šé•·ã„ã“ã¨
        if hinshi in target_hinshi and original_form_for_check not in stop_words and len(surface_form) > 1:
            words.append(surface_form)
        
        node = node.next
    return words

processed_reviews = [preprocess_text(review) for review in game_reviews]
tokenized_reviews_str = [" ".join(words) for words in processed_reviews] # å…±èµ·è¡Œåˆ—/TF-IDFç”¨

print("âœ… å‰å‡¦ç†çµæœ (å½¢æ…‹ç´ è§£æã¨ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°) ã®æœ€åˆã®5ä»¶:")
for i in range(min(5, len(processed_reviews))):
    print(f"  ãƒ¬ãƒ“ãƒ¥ãƒ¼ {i+1}: {processed_reviews[i]}")
print("-" * 50)


# --- 3. å˜èªé »å‡ºåº¦åˆ†æ (Word Frequency) ---

all_words = [word for sublist in processed_reviews for word in sublist]
word_counts = Counter(all_words)
most_common = word_counts.most_common(20) # é »å‡ºä¸Šä½20å˜èª
print("âœ… å˜èªé »å‡ºåº¦åˆ†æ (ä¸Šä½20å˜èª):")
for word, count in most_common:
    print(f"  {word}: {count}å›")

# æ£’ã‚°ãƒ©ãƒ•ã§å¯è¦–åŒ–
if most_common:
    words, counts = zip(*most_common)
    plt.figure(figsize=(12, 6))
    plt.bar(words, counts)
    plt.title('å˜èªé »å‡ºåº¦ (Word Frequency)')
    plt.xlabel('å˜èª')
    plt.ylabel('å‡ºç¾å›æ•°')
    plt.xticks(rotation=45, ha='right')
    plt.tight_layout()
    plt.savefig('word_frequency_bar_chart.png')
    plt.close()
    print("æ£’ã‚°ãƒ©ãƒ•ã‚’ 'word_frequency_bar_chart.png' ã¨ã—ã¦ä¿å­˜ã—ã¾ã—ãŸã€‚")
else:
    print("åˆ†æå¯¾è±¡ã®å˜èªãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")

print("-" * 50)


# --- 4. å…±èµ·è¡Œåˆ— (Co-occurrence Matrix) ---

co_occurrence_counts = defaultdict(int)
for words in processed_reviews:
    for i in range(len(words)):
        for j in range(i + 1, len(words)):
            pair = tuple(sorted((words[i], words[j])))
            co_occurrence_counts[pair] += 1

top_n = 10
sorted_co_occurrence = sorted(co_occurrence_counts.items(), key=lambda item: item[1], reverse=True)[:top_n]
print(f"âœ… å…±èµ·åˆ†æ (å…±èµ·é »åº¦ã®é«˜ã„ä¸Šä½{top_n}ãƒšã‚¢):")
for (word1, word2), count in sorted_co_occurrence:
    print(f"  {word1} - {word2}: {count}å›")

# TF-IDFè¡Œåˆ—ã®ä½œæˆ
vectorizer = TfidfVectorizer(use_idf=True)
tfidf_matrix = vectorizer.fit_transform(tokenized_reviews_str)
feature_names = vectorizer.get_feature_names_out()
tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names)
print("\nâœ… TF-IDFè¡Œåˆ—ã®æœ€åˆã®5è¡Œã¨5åˆ— (ãƒ‡ãƒ¼ã‚¿ã®ä¸€éƒ¨):")
print(tfidf_df.iloc[:5, :5])
print("-" * 50)


# --- 5. æ„Ÿæƒ…åˆ†æ (Sentiment Analysis) ---

def analyze_sentiment(words):
    """è¾æ›¸ãƒ™ãƒ¼ã‚¹ã®æ„Ÿæƒ…åˆ†æ"""
    positive_score = sum(1 for word in words if word in positive_words)
    negative_score = sum(1 for word in words if word in negative_words)
    
    if positive_score > negative_score:
        sentiment = 'Positive'
    elif negative_score > positive_score:
        sentiment = 'Negative'
    else:
        sentiment = 'Neutral'
        
    return sentiment, positive_score, negative_score

results = []
for i, words in enumerate(processed_reviews):
    sentiment, pos_score, neg_score = analyze_sentiment(words)
    results.append({
        'Review_ID': i + 1,
        'Original_Review': game_reviews[i],
        'Sentiment': sentiment,
        'Positive_Score': pos_score,
        'Negative_Score': neg_score
    })

sentiment_df = pd.DataFrame(results)
print("âœ… æ„Ÿæƒ…åˆ†æçµæœ (æœ€åˆã®5ä»¶):")
print(sentiment_df.head())

# æ„Ÿæƒ…æ¥µæ€§ã®åˆ†å¸ƒã‚’å¯è¦–åŒ–
sentiment_counts = sentiment_df['Sentiment'].value_counts()
if not sentiment_counts.empty:
    plt.figure(figsize=(6, 6))
    plt.pie(sentiment_counts, labels=sentiment_counts.index, autopct='%1.1f%%', startangle=90, colors=['#66b3ff','#ff9999','#99ff99'])
    plt.title('ãƒ¬ãƒ“ãƒ¥ãƒ¼æ„Ÿæƒ…æ¥µæ€§ã®åˆ†å¸ƒ')
    plt.tight_layout()
    plt.savefig('sentiment_distribution_pie_chart.png')
    plt.close()
    print("å††ã‚°ãƒ©ãƒ•ã‚’ 'sentiment_distribution_pie_chart.png' ã¨ã—ã¦ä¿å­˜ã—ã¾ã—ãŸã€‚")
else:
    print("æ„Ÿæƒ…åˆ†æã®å¯¾è±¡ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")

print("-" * 50)

# çµæœã®çµ±åˆã¨CSVæ›¸ãå‡ºã—
output_df = df.copy()
output_df = output_df.merge(sentiment_df, left_index=True, right_index=True, how='left')
output_df['Processed_Words'] = pd.Series([processed_reviews[i] if i < len(processed_reviews) else [] for i in range(len(output_df))])

output_filename = 'scenario_evaluation_results.csv'
output_df.to_csv(output_filename, index=False, encoding='utf-8')
print(f"âœ… åˆ†æçµæœã‚’ '{output_filename}' ã«æ›¸ãå‡ºã—ã¾ã—ãŸã€‚")