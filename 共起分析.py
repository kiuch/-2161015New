import pandas as pd
import MeCab
from collections import Counter, defaultdict
import matplotlib.pyplot as plt
import os
import numpy as np

# --- 1. è¨­å®šã¨è©•ä¾¡è¦³ç‚¹è¾æ›¸ã®å®šç¾© ---

# è¤‡æ•°ã®ãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã®è¨­å®š (ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒæŒ‡å®šã—ãŸçµ¶å¯¾ãƒ‘ã‚¹ã‚’ä½¿ç”¨)
file_config = [
    {'title': 'SV', 'path': r'C:\Users\masat\OneDrive\ãƒ‡ã‚¹ã‚¯ãƒˆãƒƒãƒ—\deep learning\ãƒ‘ãƒ¯ãƒ\-2161015New\SVãƒ¬ãƒ“ãƒ¥ãƒ¼æ–‡.csv', 'review_col': 'ãƒ¬ãƒ“ãƒ¥ãƒ¼'},
    {'title': 'å‰£ç›¾', 'path': r'C:\Users\masat\OneDrive\ãƒ‡ã‚¹ã‚¯ãƒˆãƒƒãƒ—\deep learning\ãƒ‘ãƒ¯ãƒ\-2161015New\å‰£ç›¾ã‚·ãƒŠãƒªã‚ªæ–‡.csv', 'review_col': 'ã‚·ãƒŠãƒªã‚ªä¸€æ–‡'},
    {'title': 'USUM', 'path': r'C:\Users\masat\OneDrive\ãƒ‡ã‚¹ã‚¯ãƒˆãƒƒãƒ—\deep learning\ãƒ‘ãƒ¯ãƒ\-2161015New\sm_usumã‚·ãƒŠãƒªã‚ªæ–‡.csv', 'review_col': 'ã‚·ãƒŠãƒªã‚ªæ–‡'}
]

# ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ (æ±ç”¨çš„ãªå˜èªã‚„ç‰¹å®šã®ã‚²ãƒ¼ãƒ ç”¨èªã‚’é™¤å»)
stop_words = {
    "ã“ã®", "ã®", "ã¯", "ãŒ", "ã«", "ã‚’", "ã¨", "ã¦", "ãŸ", "ã ", "ã—", "ã‚‚ã£ã¨", "ã‚‚", "ã§ã™", "ã¾ã™", "ã‘ã©", "ã ã‚", "ãã‚Œ", 
    "ã„ã†", "ã‚ã‚‹", "ã‚‚ã®", "ãªã‚‹", "ã™ã‚‹", "ã„ã‚‹", "ã“ã¨", "ãªã„", "ã§ãã‚‹", "ãŸã‚", "ããƒ", "ã‚‰ã‚Œã‚‹", "ã‚Œã‚‹", "ã“ã‚Œ", 
    "ã‚¹ãƒ«", "ã‚¤ãƒ«", "ã‚¤ã‚¦", "ã‚¢ãƒ«", "ãƒŠãƒ«", "ãƒŠã‚¤", "ã‚³ãƒˆ", "ãƒ‡ã‚­ãƒ«", "ã‚·ãƒ¬ãƒ«", "ã‚«ãƒ³ã‚ºãƒ«", "ãƒ¢ãƒ",
    "ã‚²ãƒ¼ãƒ ", "ã‚·ãƒªãƒ¼ã‚º", "ãƒã‚±ãƒ¢ãƒ³", "ãƒ¯ãƒ¼ãƒ«ãƒ‰", "ã‚ªãƒ¼ãƒ—ãƒ³", "ãƒ›ãƒ³ã‚µã‚¯", "ãƒ—ãƒ¬ãƒ¼ãƒ¤ãƒ¼", "ãƒ«ãƒ¼ãƒˆ", "ãƒ–ãƒ–ãƒ³", 
    "ãƒ¬ãƒ™ãƒ«", "ã‚¿ãƒ¡", "ã‚½ãƒ", "ã‚»ã‚¤ãƒªãƒ„", "ãƒˆã‚ªã‚¯", "ãƒŸã‚¨ãƒ«", "ãƒãƒ„", "ã‚¤ã‚¯", "ã‚¯ãƒ«", "ã‚ªã‚¯", "ãƒ–ã‚¿ã‚¤", "ã‚«ãƒ³ã‚±ã‚¤",
    "ãƒ›ã‚«ã‚¯", "ã‚·ãƒ¥ãƒ«ã‚¤", "ã‚¿ãƒãƒ", "ãƒãƒ", "ã‚¤ãƒ", "ã‚¢ã‚¿ãƒª", "ãƒã‚¢ã‚¤", "ã‚¸ãƒ ", "ãƒ†ãƒ©ã‚¹ã‚¿ãƒ«", "è¦ç´ ", "ã‚·ã‚¹ãƒ†ãƒ ", 
    "æ„Ÿæƒ³", "ç‚¹", "éƒ¨åˆ†", "ä»Šå›", "äºº", "æ„Ÿã˜", "æ€ã£ãŸ", "ã¨ã“ã‚", "ã¾ãŸ",
    "ã‚­ãƒ£ãƒ©" # ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ã«çµ±ä¸€ã™ã‚‹ãŸã‚é™¤å¤–
}

# 4ã¤ã®è©•ä¾¡è¦³ç‚¹ã¨è©•ä¾¡èª (æ²¡å…¥æ„Ÿã®ãƒã‚¬ãƒ†ã‚£ãƒ–è©•ä¾¡èªã‚’å¼·åŒ–)
evaluation_aspects = {
    "èµ·æ‰¿è»¢çµã®æ˜ç¢ºæ€§": {
        'aspect_words': ["å±•é–‹", "çµæœ«", "ã‚¯ãƒ©ã‚¤ãƒãƒƒã‚¯ã‚¹", "ã‚¹ãƒˆãƒ¼ãƒªãƒ¼", "æµã‚Œ", "æ§‹æˆ", "ã‚·ãƒŠãƒªã‚ª", "ä¼ç·š"],
        'positive_eval_words': ["ç´å¾—", "ã—ã£ã‹ã‚Š", "æ•´åˆæ€§", "è«–ç†çš„", "å®Œç’§", "ä¼ç·š", "è¦‹äº‹", "ç†±ã„", "é©šã", "äºˆæƒ³å¤–", "çµ‚ç›¤", "æ•´åˆæ€§"],
        'negative_eval_words': ["å°»ã™ã¼ã¿", "çŸ›ç›¾", "ä¸å®Œå…¨", "æ„å‘³ä¸æ˜", "å”çª", "é™³è…", "è–„ã„", "æµ…ã„", "ç¨šæ‹™", "ç ´ç¶»"]
    },
    "ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ã®é­…åŠ›": {
        'aspect_words': ["ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼", "ä¸»äººå…¬", "ä»²é–“", "ç™»å ´äººç‰©", "ãƒ©ã‚¤ãƒãƒ«", "å…ˆç”Ÿ", "ã‚¸ãƒ ãƒªãƒ¼ãƒ€ãƒ¼"],
        'positive_eval_words': ["é­…åŠ›çš„", "æ„Ÿæƒ…ç§»å…¥", "å€‹æ€§çš„", "æ„›ç€", "æœ€é«˜", "æ´»ãæ´»ã", "ä¸å¯§", "è‰¯ã„ã‚­ãƒ£ãƒ©", "å¥½ã"],
        'negative_eval_words': ["å¼±ã„", "è–„ã„", "å¹³å‡¡", "å€‹æ€§ãŒ", "é­…åŠ›ãŒ", "å…±æ„Ÿ", "ä¸­èº«ãŒãªã„", "é¢å€’"]
    },
    "ãƒ†ãƒ¼ãƒã®èº«è¿‘ã•": {
        'aspect_words': ["çµ†", "å‹æƒ…", "ãƒ†ãƒ¼ãƒ", "ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸", "äººé–“é–¢ä¿‚", "æˆé•·", "æ„Ÿæƒ…"],
        'positive_eval_words': ["å…±æ„Ÿ", "èº«è¿‘", "æ„Ÿå‹•", "æ·±ã„", "è€ƒãˆã•ã›ã‚‰ã‚Œã‚‹", "æš–ã‹ã„", "æ™®éçš„", "å¤§åˆ‡", "æ³£ã‘ã‚‹"],
        'negative_eval_words': ["æµ…ã„", "èª¬æ•™è‡­ã„", "ç‰©è¶³ã‚Šãªã„", "éŸ¿ã‹ãªã„", "è»½ã™ã", "çŸ­ã™ã", "è¡¨é¢ä¸Š"]
    },
    "å†’é™ºã®æ²¡å…¥æ„Ÿ": {
        'aspect_words': ["å†’é™º", "æ¢ç´¢", "æ—…", "ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰", "ä¸–ç•Œè¦³", "èˆå°", "ä½“é¨“", "ã‚ªãƒ¼ãƒ—ãƒ³ãƒ¯ãƒ¼ãƒ«ãƒ‰"],
        'positive_eval_words': ["æ²¡å…¥æ„Ÿ", "å¼•ãè¾¼ã¾ã‚Œã‚‹", "ãƒ¯ã‚¯ãƒ¯ã‚¯", "æ¥½ã—ã„", "è‡ªç”±åº¦", "ãƒªã‚¢ãƒ«", "é›°å›²æ°—", "å¿ƒèºã‚‹", "æº€å–«"],
        # ğŸŒŸ ä¿®æ­£ç‚¹: ãƒã‚¬ãƒ†ã‚£ãƒ–è©•ä¾¡èªã«ã‚·ã‚¹ãƒ†ãƒ ä¸æº€ç³»ã®å˜èªã‚’è¿½åŠ  ğŸŒŸ
        'negative_eval_words': ["å˜èª¿", "é€€å±ˆ", "ã‚¹ãƒˆãƒ¬ã‚¹", "ç§»å‹•", "ä½œæ¥­", "é ã„", "å¤‰ã‚ã‚‰ãš", "ãƒã‚°", "ã‚«ã‚¯ã‚«ã‚¯", "é‡ã„", "ãƒãƒƒãƒ—"] 
    }
}

try:
    mecab = MeCab.Tagger() 
except Exception as e:
    print(f"ğŸš¨ MeCabã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
    exit()

plt.rcParams['font.family'] = 'Meiryo' 
plt.rcParams['font.size'] = 12

# --- 2. ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•°ï¼ˆãƒ‡ãƒ¼ã‚¿å‡¦ç†ï¼‰ ---

def force_read_csv(file_path):
    """è¤‡æ•°ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’è©¦ã—ã¦CSVã‚’èª­ã¿è¾¼ã‚€"""
    encodings_to_try = ['utf-8', 'shift_jis', 'cp932', 'euc-jp']
    for encoding in encodings_to_try:
        try:
            df = pd.read_csv(file_path, encoding=encoding)
            return df
        except Exception:
            continue
    try:
        df = pd.read_csv(file_path, encoding='utf-8', errors='ignore')
        return df
    except Exception:
        return None

def unify_words(word):
    """å˜èªã‚’çµ±ä¸€ã™ã‚‹ãƒ«ãƒ¼ãƒ«"""
    if word == 'ã‚­ãƒ£ãƒ©':
        return 'ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼'
    return word

def preprocess_text(text, mecab_tagger):
    """å½¢æ…‹ç´ è§£æã€ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰é™¤å»ã€å˜èªçµ±ä¸€å‡¦ç†"""
    words = []
    if not isinstance(text, str) or len(text) < 2:
        return []
    target_hinshi = ('åè©', 'å‹•è©', 'å½¢å®¹è©', 'æ„Ÿå‹•è©')
    try:
        node = mecab_tagger.parseToNode(text)
    except Exception:
        return []
    while node:
        features = node.feature.split(',')
        hinshi = features[0]
        original_form_for_check = node.surface
        if len(features) >= 7 and features[6] != '*':
            original_form_for_check = features[6]
        surface_form = node.surface
        
        if hinshi in target_hinshi and original_form_for_check not in stop_words and len(surface_form) > 1:
            processed_word = unify_words(surface_form) 
            words.append(processed_word)
        node = node.next
    return words

def calculate_co_occurrence_score(processed_words_list):
    """4ã¤ã®è©•ä¾¡è¦³ç‚¹ã”ã¨ã®å…±èµ·åˆ†æã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—ã™ã‚‹"""
    
    aspect_scores = {}
    
    for aspect_name, definitions in evaluation_aspects.items():
        aspect_score = 0
        aspect_total_count = 0
        
        for words in processed_words_list:
            word_set = set(words)
            has_aspect_word = any(w in word_set for w in definitions['aspect_words'])
            
            if has_aspect_word:
                pos_co_occurrences = sum(1 for w in definitions['positive_eval_words'] if w in word_set)
                neg_co_occurrences = sum(1 for w in definitions['negative_eval_words'] if w in word_set)
                
                aspect_score += (pos_co_occurrences - neg_co_occurrences)
                aspect_total_count += 1
        
        if aspect_total_count > 0:
            normalized_score = aspect_score / aspect_total_count
        else:
            normalized_score = 0
        
        aspect_scores[aspect_name] = normalized_score
        
    return aspect_scores

def plot_aspect_comparison(df_aspect_scores, file_name):
    """è©•ä¾¡è¦³ç‚¹åˆ¥ã‚¹ã‚³ã‚¢ã‚’ãƒ¬ãƒ¼ãƒ€ãƒ¼ãƒãƒ£ãƒ¼ãƒˆã§å¯è¦–åŒ–ã™ã‚‹"""
    
    categories = list(df_aspect_scores.columns)
    N = len(categories)
    angles = [n / float(N) * 2 * np.pi for n in range(N)]
    angles += angles[:1]
    
    fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(polar=True))
    ax.set_theta_offset(np.pi / 2)
    ax.set_theta_direction(-1)
    
    plt.xticks(angles[:-1], categories, color='grey', size=12)
    
    # ğŸŒŸ ä¿®æ­£ç‚¹: ç›®ç››ã‚Šã®æœ€å°å€¤ã‚’0ã«å›ºå®šã—ã€è¦–è¦šçš„ãªæ¯”è¼ƒã‚’å®¹æ˜“ã«ã™ã‚‹ ğŸŒŸ
    max_val = max(df_aspect_scores.values.flatten()) * 1.2
    min_val = 0 # æœ€å°å€¤ã‚’0ã«å›ºå®š
    
    ax.set_rlabel_position(0)
    r_ticks = np.linspace(min_val, max_val, 5)
    plt.yticks(r_ticks, [f'{r:.1f}' for r in r_ticks], color="grey", size=10)
    plt.ylim(min_val, max_val)
    
    colors = ['#FF6347', '#4682B4', '#3CB371']
    titles = df_aspect_scores.index
    
    for i, title in enumerate(titles):
        # ã‚¹ã‚³ã‚¢ãŒè² ã®å ´åˆã§ã‚‚ã€ã‚°ãƒ©ãƒ•ä¸Šã¯0ã‹ã‚‰æç”»ã•ã‚Œã‚‹ã‚ˆã†ã«èª¿æ•´ãŒå¿…è¦ãªãŸã‚ã€
        # 0æœªæº€ã®å€¤ã‚’0ã«ã‚¯ãƒªãƒƒãƒ—ã—ã¦ãƒ—ãƒ­ãƒƒãƒˆã™ã‚‹ (å…ƒã®ã‚¹ã‚³ã‚¢ã¯ä¿æŒ)
        values = df_aspect_scores.loc[title].values.flatten().tolist()
        plot_values = [max(0, v) for v in values] # 0æœªæº€ã¯0ã§ãƒ—ãƒ­ãƒƒãƒˆ
        plot_values += plot_values[:1]
        
        ax.plot(angles, plot_values, linewidth=2, linestyle='solid', label=title, color=colors[i % len(colors)])
        ax.fill(angles, plot_values, color=colors[i % len(colors)], alpha=0.25)
        
    plt.title('ã‚²ãƒ¼ãƒ ã‚¿ã‚¤ãƒˆãƒ«åˆ¥ ã‚·ãƒŠãƒªã‚ªè©•ä¾¡è¦³ç‚¹ã‚¹ã‚³ã‚¢æ¯”è¼ƒ', size=16, y=1.1)
    ax.legend(loc='lower right', bbox_to_anchor=(1.25, 0.1))
    plt.savefig(file_name) 
    plt.close()
    

plt.close()

# --- 3. ãƒ¡ã‚¤ãƒ³å‡¦ç† ---

def main():
    print("ğŸš€ å…±èµ·åˆ†æï¼ˆè©•ä¾¡è¦³ç‚¹åˆ¥ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ï¼‰ã‚’é–‹å§‹ã—ã¾ã™...")

    if not os.path.exists('results'):
        os.makedirs('results')

    aspect_scores_list = []

    # --- ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã¨å‰å‡¦ç† (çµ±åˆã‚¹ãƒ†ãƒƒãƒ—) ---
    for config in file_config:
        title = config['title']
        path = config['path']
        review_col = config['review_col']
        
        print(f"\n==================== ğŸ“Š {title} ã®ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã‚’é–‹å§‹ ====================")
        
        df = force_read_csv(path)
        if df is None or review_col not in df.columns:
            print(f"ğŸš¨ ã‚¨ãƒ©ãƒ¼: {title}ã®ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã¾ãŸã¯åˆ—å'{review_col}'ã®ç¢ºèªã«å¤±æ•—ã—ã¾ã—ãŸã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
            continue

        df_game = df.copy()
        df_game = df_game.rename(columns={review_col: 'Original_Review'})
        df_game['Original_Review'] = df_game['Original_Review'].astype(str).str.strip().replace('nan', '')
        df_game = df_game[df_game['Original_Review'].str.len() > 1].reset_index(drop=True)
        game_reviews = df_game['Original_Review'].tolist()
        
        processed_words_list = [preprocess_text(review, mecab) for review in game_reviews]
        
        # --- è¦³ç‚¹åˆ¥ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ã®å®Ÿè¡Œ ---
        scores = calculate_co_occurrence_score(processed_words_list)
        scores['Game_Title'] = title
        aspect_scores_list.append(scores)
        
        print(f"âœ… {title} ã®è¦³ç‚¹åˆ¥ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—ã—ã¾ã—ãŸã€‚")

    if not aspect_scores_list:
        print("ğŸš¨ åˆ†æå¯èƒ½ãªãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚å‡¦ç†ã‚’çµ‚äº†ã—ã¾ã™ã€‚")
        return

    df_aspect_scores = pd.DataFrame(aspect_scores_list)
    df_aspect_scores.set_index('Game_Title', inplace=True)
    df_aspect_scores = df_aspect_scores[list(evaluation_aspects.keys())] 

    print("\nâœ… æœ€çµ‚çš„ãªè©•ä¾¡è¦³ç‚¹åˆ¥ã‚¹ã‚³ã‚¢ (æ­£è¦åŒ–æ¸ˆã¿):")
    print(df_aspect_scores)

    # --- ã‚°ãƒ©ãƒ•ã®å¯è¦–åŒ–ã¨ä¿å­˜ ---

    plot_aspect_comparison(df_aspect_scores, 'results/aspect_comparison_radar_chart_optimized.png')
    print("âœ… è©•ä¾¡è¦³ç‚¹åˆ¥ã‚¹ã‚³ã‚¢ã‚’ãƒ¬ãƒ¼ãƒ€ãƒ¼ãƒãƒ£ãƒ¼ãƒˆã¨ã—ã¦ä¿å­˜ã—ã¾ã—ãŸã€‚")

    output_path = 'results/aspect_scores_summary_optimized.csv'
    df_aspect_scores.to_csv(output_path, encoding='utf-8')
    print(f"âœ… è¦³ç‚¹ã‚¹ã‚³ã‚¢ã®ã‚µãƒãƒªãƒ¼ã‚’ '{output_path}' ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")

    print("\n--- å…±èµ·åˆ†æã‚¹ã‚¯ãƒªãƒ—ãƒˆã®å…¨å‡¦ç†ã‚’å®Œäº†ã—ã¾ã—ãŸ ---")

if __name__ == "__main__":
    main()